{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisation \n",
    "\n",
    "Textes en phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rosel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of text into sentences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\" Hello everyone. Hope all are fine and doing well. Hope you find the book interesting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Hello everyone.',\n",
       " 'Hope all are fine and doing well.',\n",
       " 'Hope you find the book interesting']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of text in other languages :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_tokenizer=nltk.data.load('tokenizers/punkt/french.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage collège franco-britannique de Levallois-Perret.',\n",
       " 'Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage Levallois.',\n",
       " \"L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, janvier , d'un professeur d'histoire.\",\n",
       " \"L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, mercredi , d'un professeur d'histoire\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_tokenizer.tokenize(\"Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage collège franco-britannique de Levallois-Perret. Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage Levallois. L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, janvier , d'un professeur d'histoire. L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, mercredi , d'un professeur d'histoire\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of sentences into words :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Have',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'day.',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'the',\n",
       " 'book',\n",
       " 'interesting']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Have a nice day. I hope you find the book interesting\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TreebankWordTokenizer uses conventions according to Penn Treebank Corpus. It works by \n",
    "separating contractions. This is shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "text=nltk.word_tokenize(\" Don't hesitate to ask questions\")\n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(function) def word_tokenize(\n",
    "    text: Any,\n",
    "    language: str = \"english\",\n",
    "    preserve_line: bool = False\n",
    ") -> list[str]\n",
    "\n",
    "Return a tokenized copy of *text*, using NLTK's recommended word tokenizer\n",
    "\n",
    "(currently an improved .TreebankWordTokenizer along with .PunktSentenceTokenizer for the specified language).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Another word tokenizer is PunktWordTokenizer . It works by splitting punctuation; each word is \n",
    "kept instead of creating an entirely new token. Another word tokenizer is WordPunctTokenizer . It \n",
    "provides splitting by making punctuation an entirely new token. This type of splitting is usually \n",
    "desirable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=WordPunctTokenizer()\n",
    "tokenizer.tokenize(\" Don't hesitate to ask questions\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization using regular expressions(regex)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization of words can be performed by constructing regular expressions in these two ways:\n",
    "\n",
    "• By matching with words\n",
    "\n",
    "• By matching spaces or gaps\n",
    "\n",
    "We can import RegexpTokenizer from NLTK. We can create a Regular Expression that can match \n",
    "the tokens present in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=RegexpTokenizer(\"[\\w]+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', 't', 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Don't hesitate to ask questions\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of instantiating class, an alternative way of tokenization would be to use this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'t\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "sent=\"Don't hesitate to ask questions\"\n",
    "print(regexp_tokenize(sent, pattern='\\w+|\\$[\\d\\.]+|\\S+'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion into lowercase and uppercase :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hardwork is key to success\n"
     ]
    }
   ],
   "source": [
    "text='HARdWork IS KEy to SUCCESS'\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARDWORK IS KEY TO SUCCESS\n"
     ]
    }
   ],
   "source": [
    "print(text.upper())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with stop words :\n",
    "\n",
    "NLTK has a list of stop words for many languages. We need to unzip datafile so\n",
    "that the list of stop words can be accessed from nltk_data/corpora/stopwords/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rosel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'hesitate', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "words=[\"Don't\", 'hesitate','to','ask','questions']\n",
    "w1 = [word for word in words if word not in stops]\n",
    "print(w1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instance of nltk.corpus.reader.WordListCorpusReader is a stopwords corpus. It has the words() \n",
    "function, whose argument is fileid . Here, it is English; this refers to all the stop words present in the\n",
    "English file. If the words() function has no argument, then it will refer to all the stop words of all \n",
    "the languages. Other languages in which stop word removal can be done, or the number of \n",
    "languages whose file of stop words is present in NLTK can be found using the fileids() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She must've gone to the market but she did not go.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "replacement_patterns = [\n",
    "(r'don\\'t', 'do not'),\n",
    "(r'didn\\'t', 'did not'),\n",
    "(r'can\\'t', 'cannot')\n",
    "]\n",
    "\n",
    "class RegexpReplacer(object):\n",
    "   def __init__(self, patterns=replacement_patterns):\n",
    "      self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n",
    "\n",
    "   def replace(self, text):\n",
    "      s = text\n",
    "      for (pattern, repl) in self.patterns:\n",
    "           s = re.sub(pattern, repl, s)\n",
    "      return s\n",
    "\n",
    "replacer=RegexpReplacer()\n",
    "replacer.replace(\"Don't hesitate to ask questions.\")\n",
    "print(replacer.replace(\"She must've gone to the market but she didn't go.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer= RegexpReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Don't hesitate to ask questions\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer.replace(\"Don't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"She must've gone to the market but she did not go\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacer.replace(\"She must've gone to the market but she didn't go\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function of RegexpReplacer.replace() is substituting every instance of a replacement pattern \n",
    "with its corresponding substitution pattern. Here, must've is replaced by must have and didn't is \n",
    "replaced by did not , since the replacement pattern in replacers.py has already been defined by tuple\n",
    "pairs, that is, (r'(\\w+)\\'ve', '\\g<1> have') and (r'(\\w+)n\\'t', '\\g<1> not') .\n",
    "We can not only perform the replacement of contractions; we can also substitute a token with any \n",
    "other token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer=RegexpReplacer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Don't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(replacer.replace(\"Don't hesitate to ask questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
