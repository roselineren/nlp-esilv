{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank_bm25 in c:\\python311\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (from rank_bm25) (1.26.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\rosel\\appdata\\roaming\\python\\python311\\site-packages (4.35.0)\n",
      "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\rosel\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python311\\lib\\site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rosel\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python311\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\rosel\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\rosel\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rosel\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rosel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rosel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "! pip install rank_bm25\n",
    "! pip install transformers\n",
    "#! pip install torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import urllib.request as re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import torch\n",
    "from sklearn.metrics import ndcg_score\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "nb_docs=150\n",
    "\n",
    "def loadNFCorpus():\n",
    "\tdir = \"./project1-2023/\"\n",
    "\tfilename = dir +\"dev.docs\"\n",
    "\n",
    "\tdicDoc={}\n",
    "\twith open(filename,encoding='utf-8') as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.split('\\t')\n",
    "\t\t#print(tabLine)\n",
    "\t\tkey = tabLine[0]\n",
    "\t\tvalue = tabLine[1]\n",
    "\t\t#print(value)\n",
    "\t\tdicDoc[key] = value\n",
    "\tfilename = dir + \"dev.all.queries\"\n",
    "\tdicReq={}\n",
    "\twith open(filename,encoding='utf-8') as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.split('\\t')\n",
    "\t\tkey = tabLine[0]\n",
    "\t\tvalue = tabLine[1]\n",
    "\t\tdicReq[key] = value\n",
    "\tfilename = dir + \"dev.2-1-0.qrel\"\n",
    "\tdicReqDoc=defaultdict(dict)\n",
    "\twith open(filename, encoding='utf-8') as file:\n",
    "\t\tlines = file.readlines()\n",
    "\tfor line in lines:\n",
    "\t\ttabLine = line.strip().split('\\t')\n",
    "\t\treq = tabLine[0]\n",
    "\t\tdoc = tabLine[2]\n",
    "\t\tscore = int(tabLine[3])\n",
    "\t\tdicReqDoc[req][doc]=score\n",
    "\n",
    "\treturn dicDoc, dicReq, dicReqDoc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step we run the code given as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg bm25= 0.8135524489389909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8135524489389909"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def text_to_token_list(text):\n",
    "    stopword = stopwords.words('english')\n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "    word_tokens_without_stops = [word for word in word_tokens if word not in stopword and len(word) > 2]\n",
    "    return word_tokens_without_stops\n",
    "\n",
    "def run_bm25_only(start_doc, end_doc):\n",
    "    dic_doc, dic_req, dic_req_doc = loadNFCorpus()\n",
    "\n",
    "    docs_to_keep = []\n",
    "    reqs_to_keep = []\n",
    "    dic_req_doc_to_keep = defaultdict(dict)\n",
    "\n",
    "    ndcg_top = 5\n",
    "    i = start_doc\n",
    "\n",
    "    for req_id in dic_req_doc:\n",
    "        if i > (end_doc - start_doc):\n",
    "            break\n",
    "        for doc_id in dic_req_doc[req_id]:\n",
    "            dic_req_doc_to_keep[req_id][doc_id] = dic_req_doc[req_id][doc_id]\n",
    "            docs_to_keep.append(doc_id)\n",
    "            i += 1\n",
    "        reqs_to_keep.append(req_id)\n",
    "\n",
    "    docs_to_keep = list(set(docs_to_keep))\n",
    "\n",
    "    all_vocab_doc = {}\n",
    "    for k in docs_to_keep:\n",
    "        doc_token_list = text_to_token_list(dic_doc[k])\n",
    "        for word in doc_token_list:\n",
    "            if word not in all_vocab_doc:\n",
    "                all_vocab_doc[word] = word\n",
    "\n",
    "    all_vocab_list_doc = list(all_vocab_doc)\n",
    "\n",
    "    all_vocab_req = {}\n",
    "    for k in reqs_to_keep:\n",
    "        req_token_list = text_to_token_list(dic_req[k])\n",
    "        for word in req_token_list:\n",
    "            if word not in all_vocab_req:\n",
    "                all_vocab_req[word] = word\n",
    "\n",
    "    all_vocab_list_req = list(all_vocab_req)\n",
    "\n",
    "    from rank_bm25 import BM25Okapi\n",
    "\n",
    "    corpus_doc_token_list = []\n",
    "    corpus_req_token_list = {}\n",
    "    corpus_doc_name = []\n",
    "    corpus_dico_doc_name = {}\n",
    "    i = 0\n",
    "\n",
    "    for k in docs_to_keep:\n",
    "        doc_token_list = text_to_token_list(dic_doc[k])\n",
    "        corpus_doc_token_list.append(doc_token_list)\n",
    "        corpus_doc_name.append(k)\n",
    "        corpus_dico_doc_name[k] = i\n",
    "        i += 1\n",
    "\n",
    "    corpus_req_name = []\n",
    "    corpus_dico_req_name = {}\n",
    "    i = 0\n",
    "\n",
    "    for k in reqs_to_keep:\n",
    "        req_token_list = text_to_token_list(dic_req[k])\n",
    "        corpus_req_token_list[k] = req_token_list\n",
    "        corpus_req_name.append(k)\n",
    "        corpus_dico_req_name[k] = i\n",
    "        i += 1\n",
    "\n",
    "    bm25 = BM25Okapi(corpus_doc_token_list)\n",
    "\n",
    "    ndcg_cumul = 0\n",
    "    corpus_req_vec = {}\n",
    "    ndcg_bm25_cumul = 0\n",
    "    nb_req = 0\n",
    "\n",
    "    \n",
    "    for req in corpus_req_token_list:\n",
    "        req_token_list = corpus_req_token_list[req]\n",
    "        doc_scores = bm25.get_scores(req_token_list)\n",
    "        true_docs = np.zeros(len(corpus_doc_token_list))\n",
    "\n",
    "        for doc_id in corpus_dico_doc_name:\n",
    "            if req in dic_req_doc_to_keep and doc_id in dic_req_doc_to_keep[req]:\n",
    "                pos_doc_id = corpus_dico_doc_name[doc_id]\n",
    "                true_docs[pos_doc_id] = dic_req_doc_to_keep[req][doc_id]\n",
    "\n",
    "        ndcg_bm25_cumul += ndcg_score([true_docs], [doc_scores], k=ndcg_top)\n",
    "        nb_req += 1\n",
    "\n",
    "    ndcg_bm25_cumul /= nb_req\n",
    "    print(\"ndcg bm25=\", ndcg_bm25_cumul)\n",
    "    return ndcg_bm25_cumul\n",
    "\n",
    "\n",
    "run_bm25_only(0, nb_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des approches BERT à tester "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1458b647e8441e0aeb6e01fc35f1aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rosel\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rosel\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ceee96db874ea0b28a92f9255179a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055400c76c694958b4f169d29e4de9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843ef326c76e4fc8a44c5c80ee642b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edd37febd3c4fdd88b96830626019e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_base= BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_base= BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e9eea87fc34a23b61073a5caa836c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9f2ada90084a46b63677d052b4750a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdd07ac212f4d24a43e4a335d6dd23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6edf3cd51fe4c268af436e5e981c951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/669k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2615e12e96804f0993b3066370a5c044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/691 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740bc1ca18104a6a81dc6c64c3fa45f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bio_bert_model='pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb'\n",
    "\n",
    "tokenizer_bio= BertTokenizer.from_pretrained(bio_bert_model)\n",
    "model_bio= BertModel.from_pretrained(bio_bert_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be71cee996d44f799980ce1adb701a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4c350c1ebf4ab0b76ecdcf690e21b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af9beb153bd4ba3a64ec69965baee62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520fd4b3cc774b1e977b776f62d02861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "med_bert_model='microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext'\n",
    "\n",
    "tokenizer_med= BertTokenizer.from_pretrained(med_bert_model)\n",
    "model_med= BertModel.from_pretrained(med_bert_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing\n",
    "\n",
    "### On utilise les fonctions tokenize propre à chaque modèle BERT.\n",
    "\n",
    "return_tensors='pt' : Ce paramètre indique au tokenizer de retourner des tenseurs PyTorch. Les tokens résultants seront encapsulés dans des tenseurs PyTorch.\n",
    "\n",
    "padding=True : Ce paramètre garantit que les séquences tokenizées sont remplies de zéros jusqu'à la longueur maximale dans le batch. \n",
    "\n",
    "truncation=True : Si la séquence d'entrée est plus longue que la max_length spécifiée, elle sera tronquée pour s'ajuster. En définissant truncation=True, le tokenizer tronquera les séquences qui dépassent la max_length spécifiée.\n",
    "\n",
    "max_length=512 : Ce paramètre fixe la longueur maximale des séquences. Si une séquence est plus longue que cela, elle sera tronquée ou découpée, selon le comportement du tokenizer.\n",
    "\n",
    "### Embedding: Puis on récupère un tensor correspondant à la moyenne du pooling à travers les tokens du dernier état caché du BERT model.\n",
    "\n",
    "outputs.last_hidden_state : Il s'agit de la dernière couche d'états cachés produites par le modèle BERT. Il s'agit d'un tenseur de la forme [batch_size, sequence_length, hidden_size], où hidden_size est la taille des états cachés (par exemple, 768 pour les modèles de base BERT).\n",
    "\n",
    ".mean(dim=1) : Cette opération calcule la moyenne le long de la deuxième dimension du tenseur, qui est la longueur de la séquence. Après cette opération, on obtient un tenseur de la forme [batch_size, hidden_size].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True,max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pool the token embeddings\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_bert_for_document_ranking(start_doc, end_doc, mod,tok):\n",
    "    dic_doc, dic_req, dic_req_doc = loadNFCorpus()\n",
    "    model=mod\n",
    "    tokenizer=tok\n",
    "    model.eval()\n",
    "    docs_to_keep = []\n",
    "    reqs_to_keep = []\n",
    "    dic_req_doc_to_keep = defaultdict(dict)\n",
    "\n",
    "    ndcg_top = 5\n",
    "    i = start_doc\n",
    "\n",
    "    for req_id in dic_req_doc:\n",
    "        if i > (end_doc - start_doc):\n",
    "            break\n",
    "        for doc_id in dic_req_doc[req_id]:\n",
    "            dic_req_doc_to_keep[req_id][doc_id] = dic_req_doc[req_id][doc_id]\n",
    "            docs_to_keep.append(doc_id)\n",
    "            i += 1\n",
    "        reqs_to_keep.append(req_id)\n",
    "\n",
    "    docs_to_keep = list(set(docs_to_keep))\n",
    "\n",
    "    corpus_doc_vectors = []\n",
    "    corpus_req_vectors = {}\n",
    "    corpus_doc_name = []\n",
    "    corpus_dico_doc_name = {}\n",
    "    i = 0\n",
    "\n",
    "    for k in docs_to_keep:\n",
    "        doc_text = dic_doc[k]\n",
    "        doc_embeddings = get_bert_embeddings(doc_text, model, tokenizer)\n",
    "        corpus_doc_vectors.append(doc_embeddings)\n",
    "        corpus_doc_name.append(k)\n",
    "        corpus_dico_doc_name[k] = i\n",
    "        i += 1\n",
    "\n",
    "    corpus_req_name = []\n",
    "    corpus_dico_req_name = {}\n",
    "    i = 0\n",
    "\n",
    "    for k in reqs_to_keep:\n",
    "        req_text = dic_req[k]\n",
    "        req_embeddings = get_bert_embeddings(req_text, model, tokenizer)\n",
    "        corpus_req_vectors[k] = req_embeddings\n",
    "        corpus_req_name.append(k)\n",
    "        corpus_dico_req_name[k] = i\n",
    "        i += 1\n",
    "\n",
    "    corpus_doc_vectors = torch.cat(corpus_doc_vectors, dim=0)\n",
    "\n",
    "    ndcg_bert_cumul = 0\n",
    "    nb_req = 0\n",
    "\n",
    "    for req in corpus_req_vectors:\n",
    "        true_docs = np.zeros(len(corpus_doc_vectors))\n",
    "\n",
    "        req_vector = corpus_req_vectors[req]\n",
    "        if isinstance(req_vector, torch.Tensor):\n",
    "            if req_vector.size() == (1, 768):  # Ensure req_vector has expected dimensions\n",
    "                \n",
    "\n",
    "                # Rank documents based on scores\n",
    "                ranked_docs = [(doc_id, score) for doc_id, score in zip(docs_to_keep, doc_scores)]\n",
    "                ranked_docs.sort(key=lambda x: x[1], reverse=True)  # Sort in descending order based on scores\n",
    "\n",
    "                # Compute NDCG using the ranked documents\n",
    "                for doc_id, score in ranked_docs[:ndcg_top]:\n",
    "                    if req in dic_req_doc_to_keep and doc_id in dic_req_doc_to_keep[req]:\n",
    "                        pos_doc_id = corpus_dico_doc_name[doc_id]\n",
    "                        true_docs[pos_doc_id] = dic_req_doc_to_keep[req][doc_id]\n",
    "\n",
    "                ndcg_bert_cumul += ndcg_score([true_docs], [doc_scores], k=ndcg_top)\n",
    "                nb_req += 1\n",
    "\n",
    "    ndcg_bert_cumul /= nb_req\n",
    "    print(\"NDCG for document ranking with BERT  =\", ndcg_bert_cumul)\n",
    "    return ndcg_bert_cumul\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) BERT base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG for document ranking with BERT  = 0.7852518407813259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7852518407813259"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_bert_for_document_ranking(0, nb_docs,model_base,tokenizer_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) BioBERT for sentences similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG for document ranking with BERT  = 0.7932967492743007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7932967492743007"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_bert_for_document_ranking(0, nb_docs,model_bio,tokenizer_bio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)  BioMedBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG for document ranking with BERT  = 0.9533814120500341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9533814120500341"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_bert_for_document_ranking(0, 150,model_med,tokenizer_med)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
