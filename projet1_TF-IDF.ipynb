{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "49WSBrodrf-z"
      },
      "outputs": [],
      "source": [
        "# Load the documents\n",
        "with open(\"dev.docs\", \"r\", encoding='utf-8') as f:\n",
        "    docs_content = f.readlines()\n",
        "\n",
        "# Load the queries\n",
        "with open(\"dev.all.queries\", \"r\",encoding='utf-8') as f:\n",
        "    queries_content = f.readlines()\n",
        "\n",
        "# Load the relevance scores\n",
        "with open(\"dev.2-1-0.qrel\", \"r\",encoding='utf-8') as f:\n",
        "    relevance_content = f.readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0ea3VMxxF32"
      },
      "source": [
        "Based on the samples provided:\n",
        "\n",
        "  Documents (dev.docs):\n",
        "        Each line starts with a unique identifier, followed by the content of the document.\n",
        "        The content seems to be related to medical and health topics.\n",
        "\n",
        "  Queries (dev.all.queries):\n",
        "        Each line starts with a unique identifier, followed by the query content and some associated tags.\n",
        "        The queries seem to be short phrases or questions related to medical and health topics.\n",
        "\n",
        "  Relevance Scores (dev.2-1-0.qrel):\n",
        "        Each line contains a query identifier, followed by some constant (which seems to be always 0 in the sample), a document identifier, and a relevance score.\n",
        "        The relevance score seems to be an integer (e.g., 2), possibly indicating the degree of relevance between the query and the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLqg6TuHsOUh",
        "outputId": "161dcd87-0268-49f0-eb08-b9b717e3adcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3193, 325, 324)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Parse and structure the data\n",
        "\n",
        "# Parse documents\n",
        "docs = {}\n",
        "for line in docs_content:\n",
        "    parts = line.split(\"\\t\", 1)\n",
        "    if len(parts) == 2:\n",
        "        doc_id, content = parts\n",
        "        docs[doc_id] = content.strip()\n",
        "\n",
        "# Parse queries\n",
        "queries = {}\n",
        "for line in queries_content:\n",
        "    parts = line.split(\"\\t\", 1)\n",
        "    if len(parts) == 2:\n",
        "        query_id, content = parts\n",
        "        queries[query_id] = content.strip()\n",
        "\n",
        "# Parse relevance scores\n",
        "relevance = {}\n",
        "for line in relevance_content:\n",
        "    parts = line.split(\"\\t\")\n",
        "    if len(parts) == 4:\n",
        "        query_id, _, doc_id, score = parts\n",
        "        if query_id not in relevance:\n",
        "            relevance[query_id] = {}\n",
        "        relevance[query_id][doc_id] = int(score)\n",
        "\n",
        "# Display the number of parsed documents, queries, and relevance scores\n",
        "len(docs), len(queries), len(relevance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VoD0pYTTTdNm"
      },
      "outputs": [],
      "source": [
        "#relevance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMopU8TT9QvO"
      },
      "source": [
        "The dataset has been structured as follows:\n",
        "\n",
        "  Documents: 3,193 entries\n",
        "\n",
        "  Queries: 325 entries\n",
        "  \n",
        "  Relevance Scores: 324 entries (one query seems to be missing from the relevance data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "l0VDxMuswUza"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abiwp4y-bcHF",
        "outputId": "ea2960d3-9418-40fb-86ac-9af3d96ad122"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\sraps\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEq0-wjk9cFn",
        "outputId": "3d11039d-9ce6-4e22-ff2d-e421ac25e3c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\sraps\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\sraps\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialisation du lemmatisateur et des stopwords\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Iao-c17v9gMN"
      },
      "outputs": [],
      "source": [
        "# Define a function to preprocess text\n",
        "def preprocess(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n",
        "# Fonction pour le prÃ©traitement des textes avec lemmatisation\n",
        "def preprocess_text(text):\n",
        "    # Tokenisation\n",
        "    word_tokens = word_tokenize(text.lower())\n",
        "    # Lemmatisation et suppression des stopwords\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in word_tokens if w not in stopwords.words('english') and len(w) > 2]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Preprocess documents and queries\n",
        "preprocessed_docs = {doc_id: preprocess(content) for doc_id, content in docs.items()}\n",
        "preprocessed_queries = {query_id: preprocess(content) for query_id, content in queries.items()}\n",
        "\n",
        "# Preprocess documents and queries\n",
        "preprocessed_docs_lemmatisation = {doc_id: preprocess_text(content) for doc_id, content in docs.items()}\n",
        "preprocessed_queries_lemmatisation = {query_id: preprocess_text(content) for query_id, content in queries.items()}\n",
        "\n",
        "# Display sample preprocessed data\n",
        "#list(preprocessed_docs.items())[:2], list(preprocessed_queries.items())[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9Wzn7AD09iVy"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR55Cknd-89f",
        "outputId": "e020b237-93d2-4607-aa20-e2a2d730d1ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(325, 3193)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert tokenized documents and queries back to string format for TF-IDF vectorization\n",
        "docs_string = [\" \".join(tokens) for tokens in preprocessed_docs.values()]\n",
        "queries_string = [\" \".join(tokens) for tokens in preprocessed_queries.values()]\n",
        "\n",
        "# Initialize a TF-IDF vectorizer and fit on the documents\n",
        "vectorizer = TfidfVectorizer()\n",
        "docs_tfidf = vectorizer.fit_transform(docs_string)\n",
        "\n",
        "# Transform the queries using the same vectorizer\n",
        "queries_tfidf = vectorizer.transform(queries_string)\n",
        "\n",
        "# Compute cosine similarity between queries and documents\n",
        "cosine_similarities = cosine_similarity(queries_tfidf, docs_tfidf)\n",
        "\n",
        "# Display the shape of the cosine similarities matrix\n",
        "cosine_similarities.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl6s_SP6I9Fl",
        "outputId": "ec432285-7d1f-4e47-cbec-69ea7cf97170"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.10791079, 0.03017106, 0.01821751, ..., 0.01822421, 0.05981088,\n",
              "        0.07504941],\n",
              "       [0.0988361 , 0.00767112, 0.        , ..., 0.00181114, 0.04042952,\n",
              "        0.        ],\n",
              "       [0.05368846, 0.03522577, 0.01830408, ..., 0.0237221 , 0.06460136,\n",
              "        0.06977015],\n",
              "       ...,\n",
              "       [0.01793928, 0.00487686, 0.00206799, ..., 0.0093412 , 0.        ,\n",
              "        0.        ],\n",
              "       [0.00624186, 0.        , 0.00675299, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.00420271, 0.        , 0.        , ..., 0.00156988, 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine_similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PudAQNh3IedK"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ndcg_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-Y1XTphTKTJC"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# Create a TF-IDF vectorizer for documents\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(tokens) for tokens in preprocessed_docs.values()])\n",
        "\n",
        "# Create a function to retrieve the top-k documents for a query using TF-IDF\n",
        "def retrieve_top_documents(query, k=5):\n",
        "    query_tfidf = tfidf_vectorizer.transform([query])\n",
        "    cosine_similarities = linear_kernel(query_tfidf, tfidf_matrix).flatten()\n",
        "    related_docs_indices = cosine_similarities.argsort()[::-1]\n",
        "    return [(list(preprocessed_docs.keys())[i], cosine_similarities[i]) for i in related_docs_indices][:k]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_ndcg(query_id, retrieved_docs, relevance_data):\n",
        "    if query_id in relevance_data:\n",
        "        # Extract relevance scores for retrieved documents\n",
        "        retrieved_scores = [relevance_data[query_id].get(doc_id, 0) for doc_id, _ in retrieved_docs]\n",
        "        \n",
        "        # Calculate DCG (Discounted Cumulative Gain)\n",
        "        dcg = retrieved_scores[0] + sum([(retrieved_scores[i] / np.log2(i + 2)) for i in range(1, len(retrieved_scores))])\n",
        "\n",
        "        # Sort the relevance scores in descending order to calculate ideal DCG\n",
        "        ideal_scores = [score for doc_id, score in sorted(relevance_data[query_id].items(), key=lambda x: x[1], reverse=True)]\n",
        "        idcg = ideal_scores[0] + sum([(ideal_scores[i] / np.log2(i + 2)) for i in range(1, min(len(ideal_scores), len(retrieved_scores)))])\n",
        "        \n",
        "        # Calculate NDCG\n",
        "        if idcg == 0:\n",
        "            return 0.0\n",
        "        else:\n",
        "            return dcg / idcg\n",
        "    else:\n",
        "        # Handle missing query IDs by returning a default NDCG score of 0\n",
        "        return 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average NDCG Score for all queries: 0.3551\n"
          ]
        }
      ],
      "source": [
        "# Define N_docs to limit the number of retrieved documents\n",
        "N_docs = 5\n",
        "\n",
        "# Initialize a list to store NDCG scores for all queries\n",
        "ndcg_scores = []\n",
        "\n",
        "# Loop through each query and evaluate\n",
        "for query_id, query_tokens in preprocessed_queries.items():\n",
        "    query = ' '.join(query_tokens)\n",
        "\n",
        "    # Retrieve the top N_docs documents for the query\n",
        "    retrieved_docs = retrieve_top_documents(query, k=N_docs)\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        print(f\"No relevant documents found for query {query_id}.\")\n",
        "    else:\n",
        "        # Calculate and display the NDCG score for the query based on the top N_docs documents\n",
        "        ndcg = calculate_ndcg(query_id, retrieved_docs, relevance)\n",
        "        \n",
        "\n",
        "        # Append the NDCG score to the list\n",
        "        ndcg_scores.append(ndcg)\n",
        "\n",
        "# Calculate the average NDCG score\n",
        "average_ndcg = sum(ndcg_scores) / len(ndcg_scores)\n",
        "\n",
        "# Display the average NDCG score\n",
        "print(f\"Average NDCG Score for all queries: {average_ndcg:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
